{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"M38KywBeVgUM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682248740816,"user_tz":-540,"elapsed":19746,"user":{"displayName":"권수현","userId":"11431409530480771246"}},"outputId":"4065c5f6-38e6-43de-aa65-658f3969b844"},"id":"M38KywBeVgUM","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"id":"7eb8d1c0","metadata":{"id":"7eb8d1c0"},"outputs":[],"source":["!pip install transformers\n","!pip install sentence_transformers\n","!pip install openpyxl\n","!pip install accelerate\n","!pip install bitsandbytes\n","!pip install  git+https://github.com/huggingface/peft.git\n","!pip install evaluate\n","# !pip install -v --disable-pip-version-check --no-cache-dir 'drive/MyDrive/DreamIdeaSoft data/codes/apex'"]},{"cell_type":"code","execution_count":null,"id":"82d7e905","metadata":{"id":"82d7e905"},"outputs":[],"source":["import torch\n","\n","import pandas\n","import glob\n","import logging\n","import os\n","import pickle\n","import random\n","import re\n","import shutil\n","from tqdm import tqdm as tqdm\n","from typing import Dict, List, Tuple\n","\n","import numpy as np\n","import pandas as pd\n","\n","from sklearn.model_selection import train_test_split\n","\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n","from torch.utils.data.distributed import DistributedSampler\n","from tqdm.notebook import tqdm, trange\n","\n","from pathlib import Path\n","\n","from transformers import (\n","    MODEL_WITH_LM_HEAD_MAPPING,\n","    WEIGHTS_NAME,\n","    AdamW,\n","    AutoConfig,\n","    PreTrainedModel,\n","    PreTrainedTokenizer,\n","    get_linear_schedule_with_warmup,\n",")\n","\n","\n","try:\n","    from torch.utils.tensorboard import SummaryWriter\n","except ImportError:\n","    from tensorboardX import SummaryWriter\n","\n","from transformers import AutoTokenizer, AutoModelForCausalLM \n","from transformers import AutoModelWithLMHead, AutoModelForCausalLM, AutoTokenizer\n","import torch\n","from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, TaskType, PeftModel, PeftConfig\n","from transformers import Trainer,TrainingArguments\n","from typing import Optional, Dict, Sequence\n","\n","import transformers\n","import copy\n"]},{"cell_type":"code","execution_count":5,"id":"70f95d9f","metadata":{"id":"70f95d9f","executionInfo":{"status":"ok","timestamp":1682248837386,"user_tz":-540,"elapsed":9,"user":{"displayName":"권수현","userId":"11431409530480771246"}}},"outputs":[],"source":["def data_prep_from_txt(file_name : str, o_data : dict = None) :\n","    '''\n","    Here we assume the data is a text file and follows the format :\n","    \n","    ■ location \n","    \n","    \n","    [데이터]\n","    summary....\n","    \n","    [질의응답]\n","    Q1: question 1\n","    A1: answer1 \n","    .\n","    .\n","    .\n","    \n","    '''\n","    with open(file_name,'r') as fil :\n","        data=fil.read()\n","    data=data.replace('n\\n\\n\\n\\n\\n\\n\\n\\n\\n','')\n","    data=data.split(\"■\")\n","    #print(data)\n","    data=[data[i].split('[질의응답]') for i in range(len(data))]\n","    for i in range(len(data)) :\n","        try :\n","            data[i][1]=data[i][1].split('\\nQ')\n","        except Exception as e  :\n","            continue\n","    for i in range(len(data)) :\n","        try :\n","            for j in range(len(data[i][1])) :\n","                data[i][1][j]=data[i][1][j].split('\\nA')\n","        except Exception as e  :\n","            continue\n","    data=data[1:]\n","    #print(data)\n","    for i in range(len(data)) :\n","        data[i][1]=data[i][1][1:]\n","    \n","    dataf = {} if not o_data else o_data\n","    for i in range(len(data)):\n","        dataf[data[i][0].split('\\n')[0][1:]] = {\n","               'context' : data[i][0].split('[데이터]')[1] +'\\n',\n","               'question' : ['q : ' + data[i][1][j][0].split(':')[1].replace('\\n','') +'\\na : ' for j in range(len(data[i][1]))],\n","               'answer' : [data[i][1][j][1].split(':')[1].replace('\\n','') +'\\n' for j in range(len(data[i][1]))],\n","                }  \n","        \n","    return dataf\n","\n","def make_contexted(data : dict,tokenizer) :\n","    '''\n","    Expects a dictionay of the form : \n","        dict('location':{'context' : 'summary of the locations information', \n","                         'question' : ['a list questions'],\n","                         'answer' : ['a list of answers']})\n","    '''\n","    contexted={'context' : [] ,'answer' : []}\n","    for j in data :\n","        l=[2047-tokenizer.encode(data[j]['answer'][i],return_tensors='pt').shape[1] for i in range(len(data[j]['answer']))]\n","        contexted['context'].extend([get_fitting({'context': data[j]['context'], 'question' : data[j]['question'][i]},l[i],tokenizer) for i in range(len(data[j]['answer']))])\n","        contexted['answer'].extend([data[j]['answer'][i] for i in range(len(data[j]['answer']))])\n","    contexted=pandas.DataFrame(contexted)\n","    #print(contexted)\n","    return contexted\n","\n","def data_prep_from_excel(file_name : str, o_data : dict = None ) :\n","    '''\n","    Here we assume the data is in an excel file with the columns : No, Question, Answer\n","    '''\n","    location=file_name.split('/')[-1].split('.')[0]\n","    n_data=pd.read_excel(file_name)\n","    \n","    data = {} if not o_data else o_data\n","    \n","    if location in data : \n","        data[location]['question'].extend([n_data['Question'][i] for i in range(len(n_data))])\n","        data[location]['answer'].extend([n_data['Answer'][i] for i in range(len(n_data))])\n","    \n","    \n","    else :\n","        data[location]={'context' : '' ,\n","                        'question' : [n_data['Question'][i] for i in range(len(n_data))],\n","                        'answer' : [n_data['Answer'][i] for i in range(len(n_data))]}\n","    \n","    return data\n","            \n","    "]},{"cell_type":"code","source":["from sentence_transformers import SentenceTransformer, util\n","embedder = SentenceTransformer('jhgan/ko-sroberta-multitask')\n","\n","def get_fitting(data,size,tokenizer) :\n","    hits=hits=util.semantic_search(embedder.encode(data['question']),embedder.encode(data['context'].split('\\n\\n')))\n","    x = data['context'] + data['question']\n","    l = len(hits[0])\n","    while tokenizer.encode(x,return_tensors=\"pt\").shape[1] > size :\n","        l-=1\n","        x='\\n\\n'.join([data['context'].split('\\n\\n')[hits[0][i]['corpus_id']] for i in range(l,0,-1)])+ data['question']\n","    return x\n","\n","def get_context(question : str, kD : dict) : \n","    corpus = list(kD.keys())\n","    hits = util.semantic_search(embedder.encode(question),embedder.encode(corpus))\n","    return corpus(hits[0][0]['corpus_id']) if hits[0][0][''] > 0.7 else 'unknown' "],"metadata":{"id":"YrwHuY7EPOx6"},"id":"YrwHuY7EPOx6","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":8,"id":"255f94db","metadata":{"id":"255f94db","executionInfo":{"status":"ok","timestamp":1682248867355,"user_tz":-540,"elapsed":250,"user":{"displayName":"권수현","userId":"11431409530480771246"}}},"outputs":[],"source":["def construct_conv(row, tokenizer, eos = True):\n","    flatten = lambda l: [item for sublist in l for item in sublist]\n","    conv = list(reversed([tokenizer.encode(x) + [tokenizer.eos_token_id] for x in row]))\n","    conv = flatten(conv)\n","    return conv\n","\n","class ConversationDataset(Dataset):\n","    def __init__(self, tokenizer: PreTrainedTokenizer, args, df, block_size=512):\n","\n","        block_size = block_size - (tokenizer.model_max_length - tokenizer.max_len_single_sentence)\n","\n","        directory = args.cache_dir\n","        cached_features_file = os.path.join(\n","            directory, args.model_type + \"_cached_lm_\" + str(block_size)\n","        )\n","\n","        if os.path.exists(cached_features_file) and not args.overwrite_cache:\n","            logger.info(\"Loading features from cached file %s\", cached_features_file)\n","            with open(cached_features_file, \"rb\") as handle:\n","                self.examples = pickle.load(handle)\n","        else:\n","            logger.info(\"Creating features from dataset file at %s\", directory)\n","\n","            self.examples = []\n","            for _, row in df.iterrows():\n","                conv = construct_conv(row, tokenizer)\n","                self.examples.append(conv)\n","\n","            logger.info(\"Saving features into cached file %s\", cached_features_file)\n","            with open(cached_features_file, \"wb\") as handle:\n","                pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","    def __len__(self):\n","        return len(self.examples)\n","\n","    def __getitem__(self, item):\n","        return torch.tensor(self.examples[item], dtype=torch.long)\n","\n","#### for new train\n","def preprocess(\n","    sources: Sequence[str],\n","    targets: Sequence[str],\n","    tokenizer: transformers.PreTrainedTokenizer,\n",") -> Dict:\n","    \"\"\"Preprocess the data by tokenizing.\"\"\"\n","    examples = [s + t for s, t in zip(sources, targets)]\n","    examples_tokenized, sources_tokenized = [_tokenize_fn(strings, tokenizer) for strings in (examples, sources)]\n","    input_ids = examples_tokenized[\"input_ids\"]\n","    labels = copy.deepcopy(input_ids)\n","    IGNORE_INDEX = -100\n","    for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n","        label[:source_len] = IGNORE_INDEX\n","    return dict(input_ids=input_ids, labels=labels)\n","\n","class SupervisedDataset(Dataset):\n","    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n","\n","    def __init__(self, data_df, tokenizer: transformers.PreTrainedTokenizer):\n","        super(SupervisedDataset, self).__init__()\n","        logger.warning(\"Loading data...\")\n","\n","        logger.warning(\"Formatting inputs...\")\n","        sources=[]\n","        targets=[]\n","        for _, row in data_df.iterrows():\n","            sources.append(row['context'])\n","            targets.append(row['answer'])\n","      \n","        logger.warning(\"Tokenizing inputs... This may take some time...\")\n","        data_dict = preprocess(sources, targets, tokenizer)\n","\n","        self.input_ids = data_dict[\"input_ids\"]\n","        self.labels = data_dict[\"labels\"]\n","\n","    def __len__(self):\n","        return len(self.input_ids)\n","\n","    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n","        return dict(input_ids=self.input_ids[i], labels=self.labels[i])\n","\n","class DataCollatorForSupervisedDataset(object):\n","    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n","    def __init__(self,tokenizer: transformers.PreTrainedTokenizer) :\n","        self.tokenizer=tokenizer\n","\n","    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n","        IGNORE_INDEX=-100\n","        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n","        input_ids = torch.nn.utils.rnn.pad_sequence(\n","            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n","        )\n","        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n","        return dict(\n","            input_ids=input_ids,\n","            labels=labels,\n","            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n","        )\n","\n","def make_supervised_data_module(tokenizer: transformers.PreTrainedTokenizer, data_df) -> Dict:\n","    train_dataset = SupervisedDataset(tokenizer=tokenizer, data_df=data_df)\n","    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n","    return dict(train_dataset=train_dataset, eval_dataset=None, data_collator=data_collator)\n","\n","def _tokenize_fn(strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n","    tokenized_list = [\n","        tokenizer(\n","            text,\n","            return_tensors=\"pt\",\n","            padding=\"longest\",\n","            # max_length=tokenizer.model_max_length,\n","            truncation=True,\n","        )\n","        for text in strings\n","    ]\n","    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n","    input_ids_lens = labels_lens = [\n","        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n","    ]\n","    return dict(\n","        input_ids=input_ids,\n","        labels=labels,\n","        input_ids_lens=input_ids_lens,\n","        labels_lens=labels_lens,\n","    )\n"]},{"cell_type":"code","execution_count":9,"id":"ac1f0efe","metadata":{"id":"ac1f0efe","executionInfo":{"status":"ok","timestamp":1682248870482,"user_tz":-540,"elapsed":3,"user":{"displayName":"권수현","userId":"11431409530480771246"}}},"outputs":[],"source":["def load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False):\n","    return ConversationDataset(tokenizer, args, df_val if evaluate else df_trn)\n","\n","\n","def set_seed(args):\n","    random.seed(args.seed)\n","    np.random.seed(args.seed)\n","    torch.manual_seed(args.seed)\n","    if args.n_gpu > 0:\n","        torch.cuda.manual_seed_all(args.seed)\n","\n","\n","def _sorted_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> List[str]:\n","    ordering_and_checkpoint_path = []\n","\n","    glob_checkpoints = glob.glob(os.path.join(args.output_dir, \"{}-*\".format(checkpoint_prefix)))\n","\n","    for path in glob_checkpoints:\n","        if use_mtime:\n","            ordering_and_checkpoint_path.append((os.path.getmtime(path), path))\n","        else:\n","            regex_match = re.match(\".*{}-([0-9]+)\".format(checkpoint_prefix), path)\n","            if regex_match and regex_match.groups():\n","                ordering_and_checkpoint_path.append((int(regex_match.groups()[0]), path))\n","\n","    checkpoints_sorted = sorted(ordering_and_checkpoint_path)\n","    checkpoints_sorted = [checkpoint[1] for checkpoint in checkpoints_sorted]\n","    return checkpoints_sorted\n","\n","\n","def _rotate_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> None:\n","    if not args.save_total_limit:\n","        return\n","    if args.save_total_limit <= 0:\n","        return\n","\n","    # Check if we should delete older checkpoint(s)\n","    checkpoints_sorted = _sorted_checkpoints(args, checkpoint_prefix, use_mtime)\n","    if len(checkpoints_sorted) <= args.save_total_limit:\n","        return\n","\n","    number_of_checkpoints_to_delete = max(0, len(checkpoints_sorted) - args.save_total_limit)\n","    checkpoints_to_be_deleted = checkpoints_sorted[:number_of_checkpoints_to_delete]\n","    for checkpoint in checkpoints_to_be_deleted:\n","        logger.info(\"Deleting older checkpoint [{}] due to args.save_total_limit\".format(checkpoint))\n","        shutil.rmtree(checkpoint)\n","\n"]},{"cell_type":"code","execution_count":10,"id":"a97a43da","metadata":{"id":"a97a43da","executionInfo":{"status":"ok","timestamp":1682248873159,"user_tz":-540,"elapsed":255,"user":{"displayName":"권수현","userId":"11431409530480771246"}}},"outputs":[],"source":["class Args():\n","    def __init__(self):\n","        self.output_dir = 'drive/MyDrive/DreamIdeaSoft data/codes/output-large'\n","        self.model_type = 'koGPt'\n","        self.model_name_or_path = 'kakaobrain/kogpt'\n","        self.config_name = 'kakaobrain/kogpt'\n","        self.tokenizer_name = 'kakaobrain/kogpt'\n","        self.cache_dir = 'drive/MyDrive/DreamIdeaSoft data/codes/cached'\n","        self.block_size = 2048 #tells the maximum number of tokens used per sample\n","        self.do_train = True #determines whether the model will be trained or not\n","        self.do_eval = False #determines whether the model will be evaluated or not\n","        self.evaluate_during_training = False #determine wether the model is evaluated during training or not. this can help you assess the generalizaation ability of the model during the training\n","        self.per_gpu_train_batch_size = 4 #determines the number of samples per training batch\n","        self.per_gpu_eval_batch_size = 1 #determines the number of samples per evaluation batch\n","        self.gradient_accumulation_steps = 10 #determines how_many steps over whhich the gradient is accumulated before performing back propagation and updating the weights. accumulating over more steps helps reduce the gpu memory usage during training but might make the training slower\n","        self.gradient_checkpointing = True #this makes the model save some of the activations during the forward pass in order to avoid recomputing them during the back propagation. helps reduce the gpu memory usage during training\n","        self.learning_rate = 5e-5 #sets the initial learning rate of the optimizer\n","        self.weight_decay = 0.0 #can help prevent overfiting and keep the weights low. it is a kind of regularizer\n","        self.adam_epsilon = 1e-8 #one of the hyper parameters of the optimizer. we are using adamw optimizer because it is the most comonly used for nlp(more information on adamw https://openreview.net/pdf?id=Bkg6RiCqY7, if you want to checkout other optimizers you can go here https://paperswithcode.com/methods/category/optimization)\n","        self.max_grad_norm = 1.0 #defines the maximum value of the l2 norm of the gradient. if it is greater than this the gradient will be reduced to make its norm be equal to this. this helps prevent exploding gradients which might lead to the model not learning\n","        self.num_train_epochs = 10 #defines the number of training epochs\n","        self.max_steps = -1 #sets the maximum number of steps per training epoch (-1 means there is no limit)\n","        self.warmup_steps = 0 #defines the number of warmup steps, using warmup can help tune the attention mechanism of the transformer to the data, might help improve the performance of the model\n","        self.logging_steps = 500 #the number of steps after which the training states are saved\n","        self.save_steps = 0 #number of steps after which the model is trained during the training\n","        self.save_total_limit = None #total number times the model should be saved during training\n","        self.eval_all_checkpoints = False #determines whether all checkpoints should be evaluated\n","        self.no_cuda = True \n","        self.overwrite_output_dir = True\n","        self.overwrite_cache = True\n","        self.should_continue = False\n","        self.seed = 42\n","        self.local_rank = -1\n","        self.fp16 = False #determines wether to use mixed precision training(more info https://arxiv.org/abs/1710.03740), this can speedup the training process but will require aditional libraries to be installed\n","        self.fp16_opt_level = 'O1'\n","\n","args = Args()\n"]},{"cell_type":"code","execution_count":11,"id":"22264e00","metadata":{"id":"22264e00","executionInfo":{"status":"ok","timestamp":1682248875662,"user_tz":-540,"elapsed":5,"user":{"displayName":"권수현","userId":"11431409530480771246"}}},"outputs":[],"source":["def train(args, train_dataset, model: PreTrainedModel, tokenizer: PreTrainedTokenizer) -> Tuple[int, float]:\n","    \"\"\" Train the model \"\"\"\n","    if args.local_rank in [-1, 0]:\n","        tb_writer = SummaryWriter()\n","\n","    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n","\n","    def collate(examples: List[torch.Tensor]):\n","        if tokenizer._pad_token is None:\n","            return pad_sequence(examples, batch_first=True)\n","        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n","\n","    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n","    train_dataloader = DataLoader(\n","        train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, collate_fn=collate, drop_last = True\n","    )\n","\n","    if args.max_steps > 0:\n","        t_total = args.max_steps\n","        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n","    else:\n","        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n","\n","    model = model.module if hasattr(model, \"module\") else model  # Take care of distributed/parallel training\n","    model.resize_token_embeddings(len(tokenizer))\n","    # add_special_tokens_(model, tokenizer)\n","\n","\n","    # Prepare optimizer and schedule (linear warmup and decay)\n","    no_decay = [\"bias\", \"LayerNorm.weight\"]\n","    optimizer_grouped_parameters = [\n","        {\n","            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","            \"weight_decay\": args.weight_decay,\n","        },\n","        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n","    ]\n","    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n","    )\n","\n","    # Check if saved optimizer or scheduler states exist\n","    if (\n","        args.model_name_or_path\n","        and os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\"))\n","        and os.path.isfile(os.path.join(args.model_name_or_path, \"scheduler.pt\"))\n","    ):\n","        # Load in optimizer and scheduler states\n","        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n","        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n","\n","    if args.fp16:\n","        try:\n","            from apex import amp\n","        except ImportError:\n","            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n","        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n","\n","    # multi-gpu training (should be after apex fp16 initialization)\n","    if args.n_gpu > 1:\n","        model = torch.nn.DataParallel(model)\n","\n","    # Distributed training (should be after apex fp16 initialization)\n","    if args.local_rank != -1:\n","        model = torch.nn.parallel.DistributedDataParallel(\n","            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True\n","        )\n","    if args.gradient_checkpointing :\n","      model.gradient_checkpointing_enable()\n","    # Train!\n","    logger.info(\"***** Running training *****\")\n","    logger.info(\"  Num examples = %d\", len(train_dataset))\n","    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n","    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n","    logger.info(\n","        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n","        args.train_batch_size\n","        * args.gradient_accumulation_steps\n","        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n","    )\n","    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n","    logger.info(\"  Total optimization steps = %d\", t_total)\n","\n","    global_step = 0\n","    epochs_trained = 0\n","    steps_trained_in_current_epoch = 0\n","    # Check if continuing training from a checkpoint\n","    if args.model_name_or_path and os.path.exists(args.model_name_or_path):\n","        try:\n","            # set global_step to gobal_step of last saved checkpoint from model path\n","            checkpoint_suffix = args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0]\n","            global_step = int(checkpoint_suffix)\n","            epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n","            steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n","\n","            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n","            logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n","            logger.info(\"  Continuing training from global step %d\", global_step)\n","            logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n","        except ValueError:\n","            logger.info(\"  Starting fine-tuning.\")\n","\n","    tr_loss, logging_loss = 0.0, 0.0\n","\n","    model.zero_grad()\n","    train_iterator = trange(\n","        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n","    )\n","    set_seed(args)  # Added here for reproducibility\n","    \n","    for _ in train_iterator:\n","        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n","        torch.cuda.empty_cache()\n","        for step, batch in enumerate(epoch_iterator):\n","            # Skip past any already trained steps if resuming training\n","            if steps_trained_in_current_epoch > 0:\n","                steps_trained_in_current_epoch -= 1\n","                continue\n","            torch.cuda.empty_cache()\n","            inputs, labels = (batch, batch)\n","            if inputs.shape[1] > 2048: continue\n","            inputs = inputs.to(args.device)\n","            labels = labels.to(args.device)\n","            model.train()\n","            outputs = model(inputs, labels=labels)\n","            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n","            outputs=''\n","            inputs=''\n","            labels=''\n","            torch.cuda.empty_cache()\n","            if args.n_gpu > 1:\n","                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n","            if args.gradient_accumulation_steps > 1:\n","                loss = loss / args.gradient_accumulation_steps\n","\n","            if args.fp16:\n","                with amp.scale_loss(loss, optimizer) as scaled_loss:\n","                    scaled_loss.backward()\n","            else:\n","                loss.backward()\n","\n","            tr_loss += loss.item()\n","            loss=''\n","            torch.cuda.empty_cache()\n","            if (step + 1) % args.gradient_accumulation_steps == 0:\n","                if args.fp16:\n","                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n","                else:\n","                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n","                optimizer.step()\n","                scheduler.step()  # Update learning rate schedule\n","                model.zero_grad()\n","                global_step += 1\n","\n","                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n","                    # Log metrics\n","                    if (\n","                        args.local_rank == -1 and args.evaluate_during_training\n","                    ):  # Only evaluate when single GPU otherwise metrics may not average well\n","                        results = evaluate(args, model, tokenizer)\n","                        for key, value in results.items():\n","                            tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n","                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n","                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n","                    logging_loss = tr_loss\n","\n","                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n","                    checkpoint_prefix = \"checkpoint\"\n","                    # Save model checkpoint\n","                    output_dir = os.path.join(args.output_dir, \"{}-{}\".format(checkpoint_prefix, global_step))\n","                    os.makedirs(output_dir, exist_ok=True)\n","                    model_to_save = (\n","                        model.module if hasattr(model, \"module\") else model\n","                    )  # Take care of distributed/parallel training\n","                    model_to_save.save_pretrained(output_dir)\n","                    tokenizer.save_pretrained(output_dir)\n","\n","                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n","                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n","\n","                    _rotate_checkpoints(args, checkpoint_prefix)\n","\n","                    torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n","                    torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n","                    logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n","                torch.cuda.empty_cache()\n","                     \n","            if args.max_steps > 0 and global_step > args.max_steps:\n","                epoch_iterator.close()\n","                break\n","        if args.max_steps > 0 and global_step > args.max_steps:\n","            train_iterator.close()\n","            break\n","\n","    if args.local_rank in [-1, 0]:\n","        tb_writer.close()\n","\n","    return global_step, tr_loss / global_step\n","\n","# Evaluation of some model\n","\n","def evaluate(args, model: PreTrainedModel, tokenizer: PreTrainedTokenizer, df_trn, df_val, prefix=\"\") -> Dict:\n","    # Loop to handle MNLI double evaluation (matched, mis-matched)\n","    eval_output_dir = args.output_dir\n","\n","    eval_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=True)\n","    os.makedirs(eval_output_dir, exist_ok=True)\n","    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n","    # Note that DistributedSampler samples randomly\n","\n","    def collate(examples: List[torch.Tensor]):\n","        if tokenizer._pad_token is None:\n","            return pad_sequence(examples, batch_first=True)\n","        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n","\n","    eval_sampler = SequentialSampler(eval_dataset)\n","    eval_dataloader = DataLoader(\n","        eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size, collate_fn=collate, drop_last = True\n","    )\n","\n","    # multi-gpu evaluate\n","    if args.n_gpu > 1:\n","        model = torch.nn.DataParallel(model)\n","\n","    # Eval!\n","    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n","    logger.info(\"  Num examples = %d\", len(eval_dataset))\n","    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n","    eval_loss = 0.0\n","    nb_eval_steps = 0\n","    model.eval()\n","\n","    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n","        inputs, labels = (batch, batch)\n","        inputs = inputs.to(args.device)\n","        labels = labels.to(args.device)\n","\n","        with torch.no_grad():\n","            outputs = model(inputs, labels=labels)\n","            lm_loss = outputs[0]\n","            eval_loss += lm_loss.mean().item()\n","        nb_eval_steps += 1\n","\n","    eval_loss = eval_loss / nb_eval_steps\n","    perplexity = torch.exp(torch.tensor(eval_loss))\n","\n","    result = {\"perplexity\": perplexity}\n","\n","    output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n","    with open(output_eval_file, \"w\") as writer:\n","        logger.info(\"***** Eval results {} *****\".format(prefix))\n","        for key in sorted(result.keys()):\n","            logger.info(\"  %s = %s\", key, str(result[key]))\n","            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n","\n","    return result\n","    \n","# Main runner\n","\n","def main(df_trn, df_val):\n","    args = Args()\n","    \n","    if args.should_continue:\n","        sorted_checkpoints = _sorted_checkpoints(args)\n","        if len(sorted_checkpoints) == 0:\n","            raise ValueError(\"Used --should_continue but no checkpoint was found in --output_dir.\")\n","        else:\n","            args.model_name_or_path = sorted_checkpoints[-1]\n","\n","    if (\n","        os.path.exists(args.output_dir)\n","        and os.listdir(args.output_dir)\n","        and args.do_train\n","        and not args.overwrite_output_dir\n","        and not args.should_continue\n","    ):\n","        raise ValueError(\n","            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n","                args.output_dir\n","            )\n","        )\n","\n","    # Setup CUDA, GPU & distributed training\n","    device = torch.device(\"cuda\")\n","    args.n_gpu = torch.cuda.device_count()\n","    args.device = device\n","\n","    # Setup logging\n","    logging.basicConfig(\n","        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n","        datefmt=\"%m/%d/%Y %H:%M:%S\",\n","        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n","    )\n","    logger.warning(\n","        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n","        args.local_rank,\n","        device,\n","        args.n_gpu,\n","        bool(args.local_rank != -1),\n","        args.fp16,\n","    )\n","\n","    # Set seed\n","    set_seed(args)\n","\n","    #config = AutoConfig.from_pretrained(args.config_name, cache_dir=args.cache_dir)\n","    tokenizer = AutoTokenizer.from_pretrained( 'kakaobrain/kogpt', revision='KoGPT6B-ryan1.5b-float16',  # or float32 version: revision=KoGPT6B-ryan1.5b\n","              bos_token='[BOS]', eos_token='[EOS]', unk_token='[UNK]', pad_token='[PAD]', mask_token='[MASK]')\n","   \n","    model = AutoModelForCausalLM .from_pretrained(\n","        args.model_name_or_path,\n","        revision='KoGPT6B-ryan1.5b-float16',  # or float32 version: revision=KoGPT6B-ryan1.5b\n","        pad_token_id=tokenizer.eos_token_id,\n","        torch_dtype='auto', low_cpu_mem_usage=True\n","       ).to(device='cuda', non_blocking=True)\n","\n","    # in case you already have a pretrained model in your drive you can replace the path below with the path to the model\n","    # tokenizer = AutoTokenizer.from_pretrained('drive/MyDrive/DreamIdeaSoft data/codes/kogpt')\n","    # model = AutoModelForCausalLM .from_pretrained('drive/MyDrive/DreamIdeaSoft data/codes/kogpt').to(device='cuda', non_blocking=True)\n","    \n","    logger.info(\"Training/evaluation parameters %s\", args)\n","\n","    # Training\n","    if args.do_train:\n","        train_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False)\n","\n","        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n","        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n","    input()\n","    # Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\n","    if args.do_train:\n","        # Create output directory if needed\n","        os.makedirs(args.output_dir, exist_ok=True)\n","\n","        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n","        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n","        # They can then be reloaded using `from_pretrained()`\n","        model_to_save = (\n","            model.module if hasattr(model, \"module\") else model\n","        )  # Take care of distributed/parallel training\n","        model_to_save.save_pretrained(args.output_dir)\n","        tokenizer.save_pretrained(args.output_dir)\n","\n","        # Good practice: save your training arguments together with the trained model\n","        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n","        torch.cuda.empty_cache()\n","        #Load a trained model and vocabulary that you have fine-tuned\n","        model = AutoModelForCausalLM.from_pretrained(args.output_dir)\n","        tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n","        torch.cuda.empty_cache()\n","        model.to(args.device)\n","\n","    # Evaluation\n","    results = {}\n","    if args.do_eval and args.local_rank in [-1, 0]:\n","        checkpoints = [args.output_dir]\n","        if args.eval_all_checkpoints:\n","            checkpoints = list(\n","                os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n","            )\n","            logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n","        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n","        for checkpoint in checkpoints:\n","            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n","            prefix = checkpoint.split(\"/\")[-1] if checkpoint.find(\"checkpoint\") != -1 else \"\"\n","\n","            model = AutoModelForCausalLM.from_pretrained(checkpoint)\n","            model.to(args.device)\n","            result = evaluate(args, model, tokenizer, df_trn, df_val, prefix=prefix)\n","            result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n","            results.update(result)\n","\n","    return results\n","\n"]},{"cell_type":"code","source":["import evaluate as ev\n","def train(args,train_dataset,model,tokenizer):\n","   \n","    data_module = make_supervised_data_module(tokenizer=tokenizer, data_df=train_dataset)\n","    training_args=TrainingArguments(output_dir=args.output_dir,\n","                                    gradient_accumulation_steps = args.gradient_accumulation_steps,\n","                                    weight_decay=args.weight_decay,\n","                                    learning_rate=args.learning_rate,\n","                                    adam_epsilon=args.adam_epsilon,\n","                                    max_grad_norm=args.max_grad_norm,\n","                                    warmup_steps=args.warmup_steps,\n","                                    max_steps=args.max_steps,\n","                                    gradient_checkpointing = args.gradient_checkpointing,\n","                                    per_device_train_batch_size=args.per_gpu_train_batch_size,\n","                                    optim='adamw_bnb_8bit',\n","\n","                                    fp16= args.fp16,\n","                                    num_train_epochs = args.num_train_epochs)\n","     \n","    trainer = Trainer(model=model, tokenizer=tokenizer, args=training_args, **data_module)\n","    trainer.train()\n","    trainer.model.save_pretrained(args.output_dir)\n","    trainer.tokenizer.save_pretrained(args.output_dir)\n","    #trainer.save_state()\n","    #safe_save_model_for_hf_trainer(trainer=trainer, output_dir='drive/MyDrive/DreamIdeaSoft data/codes/output-large')\n","\n","def evaluate_Exact_match(args,eval_dataset,model,tokenizer):\n","    data_module = make_supervised_data_module(tokenizer=tokenizer, data_df=eval_dataset)\n","    predictions=[]\n","    labels=[]\n","    for point in data_module['train_dataset'] :\n","        k=[j for j in point['labels'] if j == -100]\n","        labels.append(tokenizer.decode(point['labels'][len(k):],skip_special_tokens=True))\n","        model_inp=tokenizer.decode(point['input_ids'],skip_special_tokens=True)\n","        model_inp=tokenizer.encode(model_inp,return_tensors='pt').to(model.device)\n","        gen_toks=model.generate(input_ids=model_inp,\n","                                max_new_tokens=2048-model_inp.shape[-1],\n","                                num_return_sequences=1,\n","                                temperature=0.5,\n","                                no_repeat_ngram_size=6,\n","                                do_sample=True)\n","        predictions.append(tokenizer.batch_decode(gen_toks,skip_special_tokens=True)[0])\n","    metric=ev.load('exact_match')\n","    return {'exact_match' : metric.compute(references=labels,predictions=predictions)}\n","\n","def peft_model(model_name,configs) :\n","    model = AutoModelForCausalLM .from_pretrained(model_name, low_cpu_mem_usage=True,device_map='auto',load_in_8bit=True,**configs)\n","    lora_config = LoraConfig(r=8, lora_alpha=32, lora_dropout=0.1, task_type=TaskType.CAUSAL_LM)\n","    model = prepare_model_for_int8_training(model)\n","    model = get_peft_model(model, lora_config)\n","    return model\n","\n","\n","def do_train(df_trn, df_val,model=None,tokenizer=None):\n","    \n","    IGNORE_INDEX = -100\n","    device = torch.device(\"cuda\")\n","    args= Args()\n","    args.n_gpu = torch.cuda.device_count()\n","    args.device=device\n","    set_seed(args)\n","\n","    \n","    logger.info(\"Training/evaluation parameters %s\", args)\n","\n","    # Training\n","    if args.do_train:\n","        try :\n","  \n","            checkpoint=args.output_dir\n","            config=PeftConfig.from_pretrained(checkpoint)\n","            tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n","            model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,device_map='auto',load_in_8bit=True)\n","            model = PeftModel.from_pretrained(model, checkpoint, device_map={\"\":0})\n","\n","        except Exception as e :\n","            tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path,'kakaobrain/kogpt', revision='KoGPT6B-ryan1.5b-float16',\n","                  bos_token='[BOS]', eos_token='[EOS]', unk_token='[UNK]', pad_token='[PAD]', mask_token='[MASK]')\n","            configs=dict(revision='KoGPT6B-ryan1.5b-float16', pad_token_id=tokenizer.eos_token_id, torch_dtype='auto')\n","            model=peft_model(args.model_name_or_path,configs)      \n","            train(args, df_trn, model, tokenizer)\n","        \n","  \n","    if args.do_train:\n","        # Create output directory if needed\n","        os.makedirs(args.output_dir, exist_ok=True)\n","\n","        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n","        model_to_save = (\n","            model.module if hasattr(model, \"module\") else model\n","        )  # Take care of distributed/parallel training\n","        model_to_save.save_pretrained(args.output_dir)\n","        tokenizer.save_pretrained(args.output_dir)\n","        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n","        torch.cuda.empty_cache()\n","        \n","\n","    # Evaluation\n","    results = {}\n","    if args.do_eval and args.local_rank in [-1, 0]:\n","        checkpoints = [args.output_dir]  \n","        checkpoint=args.output_dir\n","        config=PeftConfig.from_pretrained(checkpoint)\n","        tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n","        model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,device_map='auto',load_in_8bit=True)\n","        model = PeftModel.from_pretrained(model, checkpoint, device_map={\"\":0})\n","        results = evaluate(args, model, tokenizer, df_trn, df_val, prefix=\"\")\n","            #\n","\n","    return results\n","\n"],"metadata":{"id":"MFUkktKnMiOM","executionInfo":{"status":"ok","timestamp":1682248882253,"user_tz":-540,"elapsed":732,"user":{"displayName":"권수현","userId":"11431409530480771246"}}},"id":"MFUkktKnMiOM","execution_count":12,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"ad53dd16","metadata":{"id":"ad53dd16"},"outputs":[],"source":["df={}\n","tokenizer = AutoTokenizer.from_pretrained( 'kakaobrain/kogpt', revision='KoGPT6B-ryan1.5b-float16',  # or float32 version: revision=KoGPT6B-ryan1.5b\n","              bos_token='[BOS]', eos_token='[EOS]', unk_token='[UNK]', pad_token='[PAD]', mask_token='[MASK]')\n","path_dir='drive/MyDrive/DreamIdeaSoft data/' #replace with the path to the txt data files\n","for fil in os.listdir(path_dir) :\n","  if not fil.endswith('txt') :\n","    continue\n","  df=data_prep_from_txt(path_dir+'/'+fil,df)\n","dt=make_contexted(df,tokenizer)\n"]},{"cell_type":"code","source":["df['팔공산 갓바위']['context']"],"metadata":{"id":"hjEJlUWk-jbq"},"id":"hjEJlUWk-jbq","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":14,"id":"f44c08ee","metadata":{"id":"f44c08ee","executionInfo":{"status":"ok","timestamp":1682248922586,"user_tz":-540,"elapsed":24,"user":{"displayName":"권수현","userId":"11431409530480771246"}}},"outputs":[],"source":["trn_df, val_df = train_test_split(dt, test_size=0.1)\n"]},{"cell_type":"code","execution_count":15,"id":"68eaaacb","metadata":{"id":"68eaaacb","executionInfo":{"status":"ok","timestamp":1682248924834,"user_tz":-540,"elapsed":251,"user":{"displayName":"권수현","userId":"11431409530480771246"}}},"outputs":[],"source":["logger = logging.getLogger(__name__)\n","\n","MODEL_CONFIG_CLASSES = list(MODEL_WITH_LM_HEAD_MAPPING.keys())\n","MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n"]},{"cell_type":"code","execution_count":43,"id":"b92f5e19","metadata":{"scrolled":true,"id":"b92f5e19","colab":{"base_uri":"https://localhost:8080/","height":105,"referenced_widgets":["9d2e01250d5f4f689e978a567993693b","c9f4df194ead4e5bb6bcc2c7f5d7ebb0","cbad5e2779e3479b923b204a549fd923","7c71d3576a164f1ba7f63ed75be56c51","ad1ee9f4a6b2471ea5f6d1cd619da206","8fc0e8e817c84644bbe193c40c8e2c93","e8a057ee4bc94c84b601bdba52a4f0cf","f651a4cc95e04d1a96c6131c586eb9e5","39d5cf88b8894812a2b4c72e65217598","dbda355e91bb416dbf39392a41773275","7377d8f351fe4a0b96f8899055b1e0b9"]},"executionInfo":{"status":"ok","timestamp":1682256351624,"user_tz":-540,"elapsed":39547,"user":{"displayName":"권수현","userId":"11431409530480771246"}},"outputId":"84655bd4-4271-4d2d-925e-8417db5b9c6d"},"outputs":[{"output_type":"stream","name":"stderr","text":["Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d2e01250d5f4f689e978a567993693b"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["{}\n"]}],"source":["### the koGPT model is an autoregressive model based on GPT-J(https://github.com/kingoflolz/mesh-transformer-jax , https://huggingface.co/EleutherAI/gpt-j-6b) which uses the transforemer architecture(https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf).\n","#main(trn_df, val_df)\n","\n","### if you have a cuda out of  memory issue instead do it loads the model where parts of the model are processed in int8 rather than float16 or float32 to reduce the space usage but might reduce in a drop in performance(more info at https://arxiv.org/pdf/2208.07339.pdf) of the model also use LoRA training where only a part of the model is trained to further reduce space consumption (you can check https://arxiv.org/pdf/2106.09685.pdf for more information LoRa)\n","do_train(trn_df,val_df)\n"]},{"cell_type":"code","source":["args=Args()\n","config=PeftConfig.from_pretrained(args.output_dir)\n","tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n","model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,device_map='auto',load_in_8bit=True)\n","model = PeftModel.from_pretrained(model, 'drive/MyDrive/DreamIdeaSoft data/codes/output-large', device_map={\"\":0})\n"],"metadata":{"id":"6bqu6yn8CAE4","colab":{"base_uri":"https://localhost:8080/","height":87,"referenced_widgets":["2864f8ee928f445cbf3be7e234e56f08","3443d50605bb48e49b5fefb7e3e6e087","a2c09611a9ce4a02bc25977eb5ab9fdd","68b3f1605266438f9fc3bc3b36f3637e","fcc4c67996b44910b48ac17f2fc13980","4a80d682ce5f41958a4e1410a00225cf","c392466fa3ab4d38befb00e24f7c8728","e06cab9cf975433da71ac2482e79f5da","73fcfa1bfa904f13a5f80a07654b151f","36861a05f3454669b596530f7f36763b","36c9d427fc504f82983fc089e9a812c4"]},"executionInfo":{"status":"ok","timestamp":1682256433443,"user_tz":-540,"elapsed":39341,"user":{"displayName":"권수현","userId":"11431409530480771246"}},"outputId":"88a6b2e5-1805-4df1-893d-ddf4700c0f41"},"id":"6bqu6yn8CAE4","execution_count":44,"outputs":[{"output_type":"stream","name":"stderr","text":["Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2864f8ee928f445cbf3be7e234e56f08"}},"metadata":{}}]},{"cell_type":"code","source":["while (1):\n","  question=input('>>')\n","  if question=='bye':\n","      break;\n","  \n","\n","# for i in range(len(dt)) :\n","  # if len(dt['context'][i]) > 3000:\n","  #   continue\n","  # input_ids=tokenizer.encode(dt['context'][i],return_tensors=\"pt\").to(model.device)\n","  context= df['팔공산 갓바위']['context']\n","  \n","  x=get_fitting({'context':context,'question':question},2048-128,tokenizer)\n","  input_ids=tokenizer.encode(x,return_tensors='pt').to('cuda')\n","  output_toks=model.generate(input_ids=input_ids,\n","                 max_new_tokens=128,\n","                 num_return_sequences=1,\n","                 temperature=0.5,\n","                 no_repeat_ngram_size=6,\n","                 do_sample=True,\n","               )\n","  print('bot>>',tokenizer.batch_decode(output_toks[:, input_ids.shape[1]:][0],skip_special_tokens=True)[0])\n"],"metadata":{"id":"AZmHzCh3DDys","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c0a3cd19-3102-42b7-e60d-976100e05986","executionInfo":{"status":"ok","timestamp":1682256218874,"user_tz":-540,"elapsed":128731,"user":{"displayName":"권수현","userId":"11431409530480771246"}}},"id":"AZmHzCh3DDys","execution_count":42,"outputs":[{"name":"stdout","output_type":"stream","text":[">>안녕\n","bot>> 하\n",">>안녕히\n","bot>> 스테리\n",">>팔공산 어디있나요\n","bot>> .\n",">>팔공산 어디있\n","bot>> 다\n",">>오랜\n","bot>>  시간\n",">>bye\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"colab":{"provenance":[]},"accelerator":"GPU","gpuClass":"premium","widgets":{"application/vnd.jupyter.widget-state+json":{"9d2e01250d5f4f689e978a567993693b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c9f4df194ead4e5bb6bcc2c7f5d7ebb0","IPY_MODEL_cbad5e2779e3479b923b204a549fd923","IPY_MODEL_7c71d3576a164f1ba7f63ed75be56c51"],"layout":"IPY_MODEL_ad1ee9f4a6b2471ea5f6d1cd619da206"}},"c9f4df194ead4e5bb6bcc2c7f5d7ebb0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8fc0e8e817c84644bbe193c40c8e2c93","placeholder":"​","style":"IPY_MODEL_e8a057ee4bc94c84b601bdba52a4f0cf","value":"Loading checkpoint shards: 100%"}},"cbad5e2779e3479b923b204a549fd923":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f651a4cc95e04d1a96c6131c586eb9e5","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_39d5cf88b8894812a2b4c72e65217598","value":2}},"7c71d3576a164f1ba7f63ed75be56c51":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dbda355e91bb416dbf39392a41773275","placeholder":"​","style":"IPY_MODEL_7377d8f351fe4a0b96f8899055b1e0b9","value":" 2/2 [00:30&lt;00:00, 13.59s/it]"}},"ad1ee9f4a6b2471ea5f6d1cd619da206":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8fc0e8e817c84644bbe193c40c8e2c93":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e8a057ee4bc94c84b601bdba52a4f0cf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f651a4cc95e04d1a96c6131c586eb9e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39d5cf88b8894812a2b4c72e65217598":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dbda355e91bb416dbf39392a41773275":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7377d8f351fe4a0b96f8899055b1e0b9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2864f8ee928f445cbf3be7e234e56f08":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3443d50605bb48e49b5fefb7e3e6e087","IPY_MODEL_a2c09611a9ce4a02bc25977eb5ab9fdd","IPY_MODEL_68b3f1605266438f9fc3bc3b36f3637e"],"layout":"IPY_MODEL_fcc4c67996b44910b48ac17f2fc13980"}},"3443d50605bb48e49b5fefb7e3e6e087":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a80d682ce5f41958a4e1410a00225cf","placeholder":"​","style":"IPY_MODEL_c392466fa3ab4d38befb00e24f7c8728","value":"Loading checkpoint shards: 100%"}},"a2c09611a9ce4a02bc25977eb5ab9fdd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e06cab9cf975433da71ac2482e79f5da","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_73fcfa1bfa904f13a5f80a07654b151f","value":2}},"68b3f1605266438f9fc3bc3b36f3637e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_36861a05f3454669b596530f7f36763b","placeholder":"​","style":"IPY_MODEL_36c9d427fc504f82983fc089e9a812c4","value":" 2/2 [00:30&lt;00:00, 13.45s/it]"}},"fcc4c67996b44910b48ac17f2fc13980":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4a80d682ce5f41958a4e1410a00225cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c392466fa3ab4d38befb00e24f7c8728":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e06cab9cf975433da71ac2482e79f5da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73fcfa1bfa904f13a5f80a07654b151f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"36861a05f3454669b596530f7f36763b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"36c9d427fc504f82983fc089e9a812c4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}